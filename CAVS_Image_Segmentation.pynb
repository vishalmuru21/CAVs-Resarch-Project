import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import matplotlib.pyplot as plt
import torchvision
import numpy as np
from google.colab import drive
import glob
import os
import sys

# Mounting the Google Drive Folder so that data set file are accessed
drive.mount('/content/drive', force_remount=True)
%cd drive/MyDrive/CAVS Dataset/MAVS_Simulated_Data
sys.path.insert(0,'content/drive/MyDrive/CAVS Dataset/MAVS_Simulated_Data/')

# Define the folder path in Google Drive
folder_data = '/content/drive/MyDrive/CAVS Dataset/MAVS_Simulated_Data/imgs'
folder_masks = '/content/drive/MyDrive/CAVS Dataset/MAVS_Simulated_Data/masks'


# Define the UNet Architecture as a Neural Network
class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()

        # Define your U-Net layers here
        # For simplicity, a basic architecture is shown
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.decoder = nn.Sequential(
            nn.Conv2d(64, 1, kernel_size=1),  # Adjust the number of output channels if needed
            nn.Sigmoid()
        )

    def forward(self, x):
        # Define the forward pass through U-Net
        x = self.encoder(x)
        x = self.decoder(x)
        return x


# This is where the CAVsDataset is defined as a class
class CAVsDataset(Dataset):
    def __init__(self, image_paths, target_paths, transform=None, mask_transform=None):
        self.image_paths = image_paths
        self.target_paths = target_paths
        self.transforms = transform
        self.mask_transforms = mask_transform

    def __getitem__(self, index):
        image = Image.open(self.image_paths[index]).convert("RGB")
        mask = Image.open(self.target_paths[index]).convert("L")  # Change to "L" for grayscale masks

        if self.transforms:
            image = self.transforms(image)
        if self.mask_transforms:
            mask = self.mask_transforms(mask)

        return image, mask

    def __len__(self):
        return len(self.image_paths)


# Create an instance of the UNet model
model = UNet()

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
])

mask_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    # Add any additional mask-specific transformations here
])

custom_dataset = CAVsDataset(
    glob.glob(os.path.join(folder_data, '*.bmp')),
    glob.glob(os.path.join(folder_masks, '*.bmp')),
    transform=transform,
    mask_transform=mask_transform
)

# Split the dataset into train and test
train_size = int(0.8 * len(custom_dataset))
test_size = len(custom_dataset) - train_size
train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])

# Create DataLoader instances
batch_size = 32  # Adjust as needed
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Train the model
num_epochs = 5
for epoch in range(num_epochs):
    for images, masks in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

  def imshow(img, title=None, figsize=(15, 5)):
    img = img / 2 + 0.5  # unnormalize
    npimg = img.numpy()
    plt.figure(figsize=figsize)
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

    if title is not None:
        plt.title(title)

    plt.show()

# It shows the images along with teh ground truth mask and the predicted masks
with torch.no_grad():
    # Display one image and its corresponding ground truth mask
    index_to_display = 0
    image, mask = custom_dataset[index_to_display]
    image = image.unsqueeze(0) # Add batch dimension
    mask = nn.functional.interpolate(mask.unsqueeze(0), size=outputs.size()[2:], mode='bilinear', align_corners=False)

    for i, (image, mask) in enumerate(test_loader):
        # Display predictions for the first sample
        if i == 0:
            prediction = model(image)

            # Display input image, ground truth mask, and predicted mask
            imshow(torchvision.utils.make_grid(image.squeeze(0)), title='Input Image', figsize=(15, 5))
            imshow(torchvision.utils.make_grid(mask.expand(-1, 3, -1, -1)), title='Ground Truth Mask', figsize=(15, 5))

            imshow(torchvision.utils.make_grid(prediction.squeeze(0)), title='Predicted Mask', figsize=(15, 5))

            plt.show()
            break

      
